{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00a09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import torch\n",
    "import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "# -custom-written libraries\n",
    "import utils\n",
    "from data.load import get_context_set\n",
    "from models.classifier import Classifier\n",
    "from models.vae import VAE\n",
    "from eval import evaluate, callbacks as cb\n",
    "from visual import visual_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0539a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable plotting in the notebook\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e3b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is used\n"
     ]
    }
   ],
   "source": [
    "# Is cuda available?\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"CUDA is {}used\".format(\"\" if cuda else \"not \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f9e43",
   "metadata": {},
   "source": [
    "## DATA: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ead555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what kind of continual learning experiment we should run\n",
    "experiment = \"splitMNIST\"  #--> create context set by splitting up the MNIST dataset\n",
    "contexts = 5               #--> split the dataset up into how many contexts?\n",
    "iters = 1000               #--> number of iterations per context\n",
    "batch = 128                #--> number of samples per iteration (i.e., the mini-batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee01ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify according to which scenario the continual learning experiment should be performed?\n",
    "scenario = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2746c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is, or should, the data be stored?\n",
    "d_dir = './store/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b6f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> MNIST: 'train'-dataset consisting of 60000 samples\n",
      " --> MNIST: 'test'-dataset consisting of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load the context set (both train- and test-data) for the specified continual learning experiment\n",
    "(train_datasets, test_datasets), config = get_context_set(\n",
    "    name=experiment, scenario=scenario, contexts=contexts, data_dir=d_dir, verbose=True, exception=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc95ea",
   "metadata": {},
   "source": [
    "## CLASSIFIER: Specify the classifier network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a212fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the architectural layout of the network to use\n",
    "fc_lay = 3        #--> number of fully-connected layers\n",
    "fc_units = 400    #--> number of units in each hidden layer\n",
    "fc_bn = False     #--> use batch-norm\n",
    "fc_nl = \"relu\"    #--> what non-linearity to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9749091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Classifier(\n",
    "    image_size=config['size'], image_channels=config['channels'], classes=config['output_units'],\n",
    "    # -conv-layers are not used\n",
    "    depth=0,\n",
    "    # -fc-layers\n",
    "    fc_layers=fc_lay, fc_units=fc_units, fc_bn=fc_bn, fc_nl=fc_nl, excit_buffer=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b15c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate to model what scenario it will be trained on and how many classes there are in each context\n",
    "model.scenario = scenario\n",
    "model.classes_per_context = config['classes_per_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate to the classifier model that it will be trained with generative replay\n",
    "model.replay_mode = 'generative'\n",
    "model.replay_targets = 'hard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749dc0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Classifier(\n",
      "  (convE): ConvLayers(\n",
      "    (pooling): Identity()\n",
      "  )\n",
      "  (flatten): Flatten()\n",
      "  (fcE): MLP(\n",
      "    (fcLayer1): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=784, out_features=400)\n",
      "      (nl): ReLU()\n",
      "    )\n",
      "    (fcLayer2): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=400)\n",
      "      (nl): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (classifier): fc_layer(\n",
      "    (linear): LinearExcitability(in_features=400, out_features=10)\n",
      "  )\n",
      ")\n",
      "-------------------------------------------------------\n",
      "--> this network has 478410 parameters (~0.5 million)\n",
      "       of which: - learnable: 478410 (~0.5 million)\n",
      "                 - fixed: 0 (~0.0 million)\n"
     ]
    }
   ],
   "source": [
    "# Print layout of the model to the screen\n",
    "utils.print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f44613",
   "metadata": {},
   "source": [
    "## GENERATOR: Specify the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184bbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the architectural layout of the network to use\n",
    "g_fc_lay = 3        #--> number of fully-connected layers\n",
    "g_fc_units = 400    #--> number of units in each hidden layer\n",
    "g_fc_bn = False     #--> use batch-norm\n",
    "g_fc_nl = \"relu\"    #--> what non-linearity to use?\n",
    "g_z_dim = 100       #--> number of units in latent space VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d0ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "generator = VAE(\n",
    "    image_size=config['size'], image_channels=config['channels'],\n",
    "    # -conv-layers are not used\n",
    "    depth=0,\n",
    "    # -fc-layers\n",
    "    fc_layers=g_fc_lay, fc_units=g_fc_units, fc_bn=g_fc_bn, fc_nl=g_fc_nl, excit_buffer=True,\n",
    "    # -prior\n",
    "    prior='standard', z_dim=g_z_dim,\n",
    "    #-decoder\n",
    "    recon_loss='BCE', network_output='sigmoid'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e1d1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "VAE(\n",
      "  (convE): ConvLayers(\n",
      "    (pooling): Identity()\n",
      "  )\n",
      "  (flatten): Flatten()\n",
      "  (fcE): MLP(\n",
      "    (fcLayer1): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=784, out_features=400)\n",
      "      (nl): ReLU()\n",
      "    )\n",
      "    (fcLayer2): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=400)\n",
      "      (nl): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (toZ): fc_layer_split(\n",
      "    (mean): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=100)\n",
      "    )\n",
      "    (logvar): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=100)\n",
      "    )\n",
      "  )\n",
      "  (fromZ): fc_layer(\n",
      "    (linear): LinearExcitability(in_features=100, out_features=400)\n",
      "    (nl): ReLU()\n",
      "  )\n",
      "  (fcD): MLP(\n",
      "    (fcLayer1): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=400)\n",
      "      (nl): ReLU()\n",
      "    )\n",
      "    (fcLayer2): fc_layer(\n",
      "      (linear): LinearExcitability(in_features=400, out_features=784)\n",
      "      (nl): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (to_image): Reshape(channels = 1)\n",
      "  (convD): DeconvLayers()\n",
      ")\n",
      "-------------------------------------------------------\n",
      "--> this network has 1069684 parameters (~1.1 million)\n",
      "       of which: - learnable: 1069684 (~1.1 million)\n",
      "                 - fixed: 0 (~0.0 million)\n"
     ]
    }
   ],
   "source": [
    "# Print layout of the generator to the screen\n",
    "utils.print_model_info(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448262e",
   "metadata": {},
   "source": [
    "## PREPARE FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680da4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the optimizer to use\n",
    "lr = 0.001           #--> learning rate to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e3602ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the classifier model, set up the optimizer to use\n",
    "model.optim_list = [{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': lr}]\n",
    "model.optimizer = torch.optim.Adam(model.optim_list, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da254942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generative model, set up the optimizer to use\n",
    "generator.optim_list = [{'params': filter(lambda p: p.requires_grad, generator.parameters()), 'lr': lr}]\n",
    "generator.optimizer = torch.optim.Adam(generator.optim_list, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de6d49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for keeping track of performance (on test set) during training, for later plotting\n",
    "plotting_dict = evaluate.initiate_plotting_dict(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "242bbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function for keeping track of performance trhoughout training\n",
    "eval_periodicity = 50\n",
    "eval_cb = cb._eval_cb(log=eval_periodicity, test_datasets=test_datasets, plotting_dict=plotting_dict,\n",
    "                      visdom=None, iters_per_context=iters, test_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94eb7ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the models in training mode\n",
    "_ = model.train()\n",
    "_ = generator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf0d2d",
   "metadata": {},
   "source": [
    "## TRAIN: Train the classifier and generative model on the continual learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<CLASSIFIER> | Context: 1/5 | training loss: 1.02e-06 | training accuracy: 1.0 |: 100%|████████████████████████████| 1000/1000 [00:28<00:00, 35.00it/s]\n",
      "<CLASSIFIER> | Context: 2/5 | training loss: 0.00302 | training accuracy: 1.0 |: 100%|█████████████████████████████| 1000/1000 [00:35<00:00, 27.94it/s]\n",
      "<CLASSIFIER> | Context: 3/5 | training loss: 0.0424 | training accuracy: 1.0 |: 100%|██████████████████████████████| 1000/1000 [00:36<00:00, 27.69it/s]\n",
      "<CLASSIFIER> | Context: 4/5 | training loss: 0.0366 | training accuracy: 1.0 |: 100%|██████████████████████████████| 1000/1000 [00:36<00:00, 27.38it/s]\n",
      "<CLASSIFIER> | Context: 5/5 | training loss: 0.268 | training accuracy: 0.844 |:  18%|█████▍                        | 180/1000 [00:06<00:29, 27.64it/s]"
     ]
    }
   ],
   "source": [
    "# Indicate that on first context, there are no stored models yet for generating replay\n",
    "previous_model = previous_generator = None\n",
    "    \n",
    "# Loop over all contexts.\n",
    "for context, train_dataset in enumerate(train_datasets, 1):\n",
    "    \n",
    "    # Find [active_classes]\n",
    "    if scenario==\"task\":\n",
    "        # -for Task-IL scenario, create <list> with for all contexts so far a <list> with the active classes\n",
    "        active_classes = [list(\n",
    "            range(model.classes_per_context * i, model.classes_per_context * (i+1))\n",
    "        ) for i in range(context)]\n",
    "    else:\n",
    "        # -for Domain- and Class-IL scenario, always all classes are active\n",
    "        active_classes = None\n",
    "    \n",
    "    # Initialize # iters left on current data-loader(s)\n",
    "    iters_left = 1\n",
    "    \n",
    "    # Define tqdm progress bar\n",
    "    progress = tqdm.tqdm(range(1, iters+1))\n",
    "    \n",
    "    # Loop over all iterations\n",
    "    for batch_index in range(1, iters+1):\n",
    "        \n",
    "        # Update # iters left on current data-loader(s) and, if needed, create new one(s)\n",
    "        iters_left -= 1\n",
    "        if iters_left==0:\n",
    "            data_loader = iter(utils.get_data_loader(train_dataset, batch, cuda=cuda, drop_last=True))\n",
    "            iters_left = len(data_loader)\n",
    "            \n",
    "        # -----------------Collect data------------------#\n",
    "\n",
    "        #--> Sample training data of current context\n",
    "        x, y = next(data_loader)\n",
    "        # Adjust y-targets to 'active range'\n",
    "        y = y-model.classes_per_context*(context-1) if scenario=='task' else y\n",
    "        # Transfer them to correct device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        #--> Sample inputs to be replayed from stored copy of generative model\n",
    "        if previous_model is not None:\n",
    "            x_ = previous_generator.sample(batch, only_x=True)\n",
    "            if not scenario=='task':\n",
    "                with torch.no_grad():\n",
    "                    scores_ = previous_model.classify(x_)\n",
    "                _, y_ = torch.max(scores_, dim=1)\n",
    "            else:\n",
    "                # With Task-IL, [x_] needs to be evaluated according to each past context\n",
    "                scores_ = list()\n",
    "                y_ = list()\n",
    "                with torch.no_grad():\n",
    "                    all_scores_ = previous_model.classify(x_)\n",
    "                for context_id in range(context-1):\n",
    "                    temp_scores_ = all_scores_[:, active_classes[context_id]]\n",
    "                    scores_.append(temp_scores_)\n",
    "                    _, temp_y_ = torch.max(temp_scores_, dim=1)\n",
    "                    y_.append(temp_y_)\n",
    "\n",
    "            # Only keep predicted y/scores if required (as otherwise unnecessary computations will be done)\n",
    "            y_ = y_ if (model.replay_targets == \"hard\") else None\n",
    "            scores_ = scores_ if (model.replay_targets == \"soft\") else None\n",
    "        else:\n",
    "            x_ = y_ = scores_ = None    #-> when training on the first context, there is no replay yet\n",
    "        \n",
    "        # Train the classifier model on this batch\n",
    "        loss_dict = model.train_a_batch(\n",
    "            x, y, x_=x_, y_=y_, scores_=scores_, rnt = 1./context,\n",
    "            active_classes=active_classes, context=context\n",
    "        )\n",
    "            \n",
    "        # Train the generative model on this batch\n",
    "        _ = generator.train_a_batch(x, x_=x_, rnt=1./context)\n",
    "        \n",
    "        # Update progress bar\n",
    "        context_stm = \" Context: {}/{} |\".format(context, contexts)\n",
    "        progress.set_description(\n",
    "            '<CLASSIFIER> |{t_stm} training loss: {loss:.3} | training accuracy: {prec:.3} |'\n",
    "                .format(t_stm=context_stm, loss=loss_dict['loss_total'], prec=loss_dict['accuracy'])\n",
    "        )\n",
    "        progress.update(1)\n",
    "        \n",
    "        # Execute callback-function to keep track of performance throughout training\n",
    "        if eval_cb is not None:\n",
    "            eval_cb(model, batch_index, context=context)\n",
    "    \n",
    "    ##----------> UPON FINISHING EACH CONTEXT...\n",
    "\n",
    "    # Close progres-bar\n",
    "    progress.close()\n",
    "    \n",
    "    # Update the source for the replay\n",
    "    previous_generator = copy.deepcopy(generator).eval()\n",
    "    previous_model = copy.deepcopy(model).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcf17f",
   "metadata": {},
   "source": [
    "## EVALUATION: average accuracy throughout training and samples from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 25\n",
    "sample = generator.sample(size)\n",
    "image_tensor = sample.view(-1, config['channels'], config['size'], config['size']).cpu()\n",
    "nrow = int(np.ceil(np.sqrt(size)))\n",
    "visual_plt.plot_images_from_tensor(image_tensor, title='Samples from generative model', nrow=nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = []\n",
    "for i in range(contexts):\n",
    "    plot_list.append(plotting_dict[\"acc per context\"][\"context {}\".format(i + 1)])\n",
    "figure = visual_plt.plot_lines(\n",
    "    plot_list, x_axes=plotting_dict[\"x_iteration\"],\n",
    "    line_names=['Context {}'.format(i + 1) for i in range(contexts)],\n",
    "    ylabel=\"Test accuracy (per context)\", ylim=(0,1.05) if scenario==\"class\" else None,\n",
    "    xlabel=\"Iteration\", title=\"{}  --  {}-incremental learning\".format(experiment, scenario)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d2f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
